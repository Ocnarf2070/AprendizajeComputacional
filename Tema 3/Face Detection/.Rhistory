length(x1)
for(i in 1:length(x1)){
if(sqrt(x1[i]^2+x2[i]^2)>2){
x1[i]<-4-x2+abs(x1-x2)
x2[i]<-4-x1+abs(x1-x2)
}
}
#d)--------------------------------------------------
#Transformation
x1<-c(2,2,-2,-2,2,2,-2,-2,1,1,-1,-1)
x2<-c(2,-2,-2,2,2,-2,-2,2,1,-1,-1,1)
y<-c(1,1,1,1,1,1,1,1,-1,-1,-1,-1)
for(i in 1:length(x1)){
if(sqrt(x1[i]^2+x2[i]^2)>2){
x1[i]<-4-x2+abs(x1-x2)
x2[i]<-4-x1+abs(x1-x2)
}
}
rm(list = ls())
#d)--------------------------------------------------
#Transformation
x1<-c(2,2,-2,-2,2,2,-2,-2,1,1,-1,-1)
x2<-c(2,-2,-2,2,2,-2,-2,2,1,-1,-1,1)
y<-c(1,1,1,1,1,1,1,1,-1,-1,-1,-1)
for(i in 1:length(x1)){
if(sqrt(x1[i]^2+x2[i]^2)>2){
x1[i]<-4-x2+abs(x1-x2)
x2[i]<-4-x1+abs(x1-x2)
}
}
debugSource('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 2/SMV/SVM(1).R')
sqrt(x1[i]^2+x2[i]^2)>2
for(i in 1:length(x1)){
if(sqrt(x1[i]^2+x2[i]^2)>2){
x1[i]<-4-x2[i]+abs(x1[i]-x2[i])
x2[i]<-4-x1[i]+abs(x1[i]-x2[i])
}
}
x1
x2
Data4<-data.frame(x1,x2,y)
x<-cbind(Data4$x1, Data4$x2)
y <- Data4$y
n0 <- sum(y=='1')
n1 <- sum(y=='-1')
colores <- c(rep("green",n0), rep("red",n1))
pchn <- 21
# Diagrama de dispersion
plot(x, pch = pchn, bg = colores)
#Formula, SVM y datos importantes
formula<-as.factor(y)~.
svm.lineal3 <- svm (formula, data=Data4,kernel = "linear", cost = 1000, scale = FALSE)
x.svm <-x [svm.lineal3$index,]
w <- crossprod(x.svm, svm.pol$coefs)
svm.pol$coefs
w <- crossprod(x.svm, svm.lineal3$coefs)
w0 <- svm.lineal3$rho
#Vectores Soporte
summary(svm.lineal3)
#Valores del kernel
A<-x[svm.lineal3$index[1],]
B<-x[svm.lineal3$index[2],]
text<-sprintf("K(A,A)=%.2f\n",crossprod(A,A))
cat(text)
text<-sprintf("K(A,B)=%.2f\n",crossprod(A,B))
cat(text)
text<-sprintf("K(B,A)=%.2f\n",crossprod(B,A))
cat(text)
text<-sprintf("K(B,B)=%.2f\n",crossprod(B,B))
cat(text)
#Ancho de banda
text<-sprintf("Ancho de banda: %.5f\n",2/norm(w,type = '2'))
cat(text)
#Vector de pesos normal al hiperplano
text<-sprintf("Vector de pesos normal al hiperplano (W): (%s)\n",paste(w, collapse=", "))
cat(text)
# Termino independiente
text<-sprintf("Termino independiente (B): %.2f\n",w0)
cat(text)
#Ecuaciones de hiperplano
text<-sprintf("Hiperplanos:\n%.5fx+%.5fy+%.2f=0\n",w[1],w[2],-w0)
cat(text)
text<-sprintf("%.5fx+%.5fy+%.2f=-1\n",w[1],w[2],-w0)
cat(text)
text<-sprintf("%.5fx+%.5fy+%.2f=1\n",w[1],w[2],-w0)
cat(text)
lim<-x+c(-2,2)
plot(x, pch = pchn, bg = colores,xlim = c(min(lim[,1]), max(lim[,1])), ylim =c(min(lim[,2]), max(lim[,2])) )
abline(w0/w[2], -w[1]/w[2])
abline((w0 - 1) / w[2], -w[1] / w[2], lty = 2)
abline((w0 + 1) / w[2], -w[1] / w[2], lty = 2)
readline(prompt="Press [enter] to continue")
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 2/SMV/SVM(1).R')
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 2/SMV/SVM(1).R')
#Vectores Soporte
view(summary(svm.lineal4))
#Vectores Soporte
show(summary(svm.lineal4))
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 2/SMV/SVM(1).R')
show(summary(svm.pol))
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 2/SMV/SVM(1).R')
ççççççççççççççççççççççççççççççççççççççççççççççççççççççççççççççççççç
rm(list = ls())
library(MASS)
library(e1071)
#a)--------------------------------------------------
x1<-c(0,4)
x2<-c(0,4)
y<-c(1,-1)
Data1<-data.frame(x1,x2,y)
x<-cbind(Data1$x1, Data1$x2)
y <- Data1$y
n0 <- sum(y=='1')
n1 <- sum(y=='-1')
colores <- c(rep("green",n0), rep("red",n1))
pchn <- 21
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 2/SMV/SVM(1).R')
setwd("D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 3/Face Detection")
rm(list = ls())
library(randomForest)
library(pROC)
library(rpart)
library(adabag)
library(gbm)
data <- read.csv("training/training.csv", stringsAsFactors = F)
data.update <- na.omit(data)
im.train <- data.update$Image
data.update$Image <- NULL
library("doParallel")
registerDoParallel()
im.train <- foreach(im = im.train, .combine=rbind) %dopar% {
as.integer(unlist(strsplit(im, " ")))
}
im.train <- as.data.frame(im.train)
View(im.train)
im <- matrix(data=rev(im.train[1,]), nrow=96, ncol=96)
image(1:96, 1:96, im, col=gray((0:255)/255))
data.update <- na.omit(data)
im.train <- data.update$Image
data.update$Image <- NULL
im.train <- foreach(im = im.train, .combine=rbind) %dopar% {
as.integer(unlist(strsplit(im, " ")))
}
im <- matrix(data=rev(im.train[1,]), nrow=96, ncol=96)
image(1:96, 1:96, im, col=gray((0:255)/255))
data <- cbind(data.update, im.train)
im.train <- as.data.frame(im.train)
data <- cbind(data.update, im.train)
ind <- sample(2, nrow(data), replace=TRUE, prob=c(0.8, 0.2))
train <- data.update[ind==1,]
test <- data.update[ind==2,]
## Random Forest
arbolx <- randomForest(nose_tip_x~., train,importance=TRUE)
arboly <- randomForest(nose_tip_y~., train,importance=TRUE)
importance(arbolx)
importance(arboly)
predRFx <- predict(arbolx, test)
predRFx <- as.data.frame(predRFx)
View(predRFx)
table(predRFx,test$nose_tip_x)
predRFx <- predict(arbolx, test)
table(predRFx,test$nose_tip_x)
predRFx <- predict(arbolx, test)
#predRFx <- as.data.frame(predRFx)
confusionMatrixRF<-table(predRFx, test$nose_tip_x)
accuracyRF<-sum(diag(confusionMatrixRF))/sum(confusionMatrixRF)
accuracyRF<-sum(diag(confusionMatrixRF))/sum(confusionMatrixRF);accuracyRF
predRFy <- predict(arboly, test)
#predRFy <- as.data.frame(predRFy)
confusionMatrixRF<-table(predRFy, test$nose_tip_x)
accuracyRF<-sum(diag(confusionMatrixRF))/sum(confusionMatrixRF);accuracyRF
cm <- cbind(predRFx, truex = test$nose_tip_x)
predRFx <- as.data.frame(predRFx)
predRFy <- as.data.frame(predRFy)
cm <- cbind(predRFx, truex = test$nose_tip_x)
View(cm)
cm <- cbind(cm, MAPEx = (abs(cm$predRFx-cm$truex)/cm$truex)*100)
cm <- cbind(cm, predRFy)
cm <- cbind(cm, truey = test$nose_tip_y)
cm <- cbind(cm, MAPEy = (abs(cm$predRFy-cm$truey)/cm$truey)*100)
(abs(cm$predRFy-cm$truey)/cm$truey)
((cm$truey-cm$predRFy)^2)
cm <- cbind(cm, MSEx = ((cm$truex-cm$predRFx)^2))
cm <- cbind(cm, MSEy = ((cm$truey-cm$predRFy)^2))
MAPE_PROMEDIORFX <- sum(cm$MAPEx)/nrow(cm)
MAPE_PROMEDIORFY <- sum(cm$MAPEy)/nrow(cm)
MAPE_PROMEDIORFX
MAPE_PROMEDIORFY
MSE_MEANx <- mean(cm$MSEx)
MAPE_PROMEDIORFX
MSE_MEANx
MAPE_PROMEDIORFY
MAPE_PROMEDIORFY <- sum(cm$MSEy)/nrow(cm)
MAPE_PROMEDIORFY
name(test$nose_tip_x)
names(test$nose_tip_x)
names(test)
attributes(test)
which(test %in% test$nose_tip_x)
which(test$name %in% test$nose_tip_x)
cat("MSE for the nose_tip_y: ",MSE_MEANx,"\n")
## Adaboost
# Because we cannot use directly adaboots for regretions, we must use gbm, which is the adaboots for regretions
adaX <- gbm(nose_tip_x~., data=train, distribution="adaboost", cv.folds = 3)
adaY <- gbm(nose_tip_y~., data=train, distribution="adaboost", cv.folds = 3)
## Adaboost
# Because we cannot use directly adaboots for regretions, we must use gbm, which is the adaboots for regretions
adaX <- gbm(nose_tip_x~., data=train, distribution="gaussian", cv.folds = 3)
adaY <- gbm(nose_tip_y~., data=train, distribution="gaussian", cv.folds = 3)
best.iterX <- gbm.perf(adaX,method="cv")
best.iterY <- gbm.perf(adaY,method="cv")
predAdax <- predict(adaX, test, best.iterX)
predAday <- predict(adaY, test, best.iterY)
predAdax <- as.data.frame(predAdax)
predAday <- as.data.frame(predAday)
cmAda <- cbind(predAdax, truex = test$nose_tip_x)
cmAda <- cbind(cmAda, MSEx = ((cmAda$truex-cmAda$predAdax)^2))
cmAda <- cbind(cmAda,predAday)
cmAda <- cbind(cmAda, truey = test$nose_tip_y)
cmAda <- cbind(cmAda, MSEy = ((cmAda$truey-cmAda$predAday)^2))
MSE_MEAN_ADAx <- mean(cm_rf$MSEx)
MSE_MEAN_ADAx <- mean(cmAda$MSEx)
MSE_MEAN_ADAy <- mean(cmAda$MSEy)
cat("MSE for the nose_tip_x for Adaboost: ",MSE_MEAN_ADAx,"\n")
cat("MSE for the nose_tip_y for Adaboost: ",MSE_MEAN_ADAy,"\n")
install.packages("ipred")
install.packages("ipred")
##Bagging
#Because we cannot use diectly bagging from package adabag, so we will use
# the package ipred for the bagging
library(ipred)
install.packages("ipred")
##Bagging
#Because we cannot use diectly bagging from package adabag, so we will use
# the package ipred for the bagging
library(ipred)
BagX<-bagging(nose_tip_x~.,data = train)
predBagx <- predict(BagX, test)
predBagx <- as.data.frame(predAdax)
BagY<-bagging(nose_tip_y~.,data = train)
predBagy <- predict(BagY, test)
predBagy <- as.data.frame(predAday)
cmBag <- cbind(predBagx, truex = test$nose_tip_x)
cmBag <- cbind(cmBag, MSEx = ((cmAda$truex-cmAda$predBagx)^2))
View(cmBag)
((cmAda$truex-cmAda$predBagx)^2)
((cmAda$truex-cmAda$predBagx))
cmAda$predBagx
cmBag <- cbind(predBagx, truex = test$nose_tip_x)
cmBag <- cbind(cmBag, MSEx = ((cmBag$truex-cmBag$predBagx)^2))
((cmBag$truex-cmBag$predBagx)^2)
cmBag$predBagx
rm(list = ls())
library(randomForest)
library(pROC)
library(gbm)
library(rpart)
data <- read.csv("training/training.csv", stringsAsFactors = F)
data.update <- na.omit(data)
im.train <- data.update$Image
data.update$Image <- NULL
library("doParallel")
registerDoParallel()
im.train <- foreach(im = im.train, .combine=rbind) %dopar% {
as.integer(unlist(strsplit(im, " ")))
}
im.train <- as.data.frame(im.train)
data <- cbind(data.update, im.train)
ind <- sample(2, nrow(data), replace=TRUE, prob=c(0.8, 0.2))
train <- data.update[ind==1,]
test <- data.update[ind==2,]
##Bagging
#Because we cannot use diectly bagging from package adabag, so we will use
# the package ipred for the bagging
library(ipred)
BagX<-bagging(nose_tip_x~.,data = train)
BagY<-bagging(nose_tip_y~.,data = train)
predBagx <- predict(BagX, test)
predBagy <- predict(BagY, test)
predBagx <- as.data.frame(predAdax)
predBagy <- as.data.frame(predAday)
predBagx <- as.data.frame(predBagx)
predBagy <- as.data.frame(predBagy)
cmBag <- cbind(predBagx, truex = test$nose_tip_x)
cmBag <- cbind(cmBag, MSEx = ((cmBag$truex-cmBag$predBagx)^2))
cmBag <- cbind(cmBag,predBagy)
cmBag <- cbind(cmBag, truey = test$nose_tip_y)
cmBag <- cbind(cmBag, MSEy = ((cmBag$truey-cmBag$predBagy)^2))
MSE_MEAN_ADAx <- mean(cmBag$MSEx)
MSE_MEAN_ADAy <- mean(cmBag$MSEy)
cat("MSE for the nose_tip_x for Adaboost: ",MSE_MEAN_ADAx,"\n")
cat("MSE for the nose_tip_y for Adaboost: ",MSE_MEAN_ADAy,"\n")
perceptronX<-nnet(nose_tip_x~., train, size=3)
##Stacking
library(nnet)
library(e1071)
perceptronX<-nnet(nose_tip_x~., train, size=3)
treeX<-rpart(nose_tip_x~., train)
install.packages("neuralnet")
##Stacking
library(neuralnet)
nnX<-neuralnet(nose_tip_x~., data = train, err.fct = "sse")
rm(list = ls())
library(randomForest)
library(pROC)
library(rpart)
library(gbm)
data <- read.csv("training/training.csv", stringsAsFactors = F)
data.update <- na.omit(data)
im.train <- data.update$Image
data.update$Image <- NULL
library("doParallel")
registerDoParallel()
im.train <- foreach(im = im.train, .combine=rbind) %dopar% {
as.integer(unlist(strsplit(im, " ")))
}
im.train <- as.data.frame(im.train)
data <- cbind(data.update, im.train)
ind <- sample(2, nrow(data), replace=TRUE, prob=c(0.8, 0.2))
train <- data.update[ind==1,]
test <- data.update[ind==2,]
##Stacking
library(neuralnet)
library(e1071)
nnX<-neuralnet(nose_tip_x~., data = train, err.fct = "sse")
n <- names(train)
fx <- as.formula(paste("nose_tip_x ~", paste(n[!n %in% "nose_tip_x"], collapse = " + ")))
fx
nnX<-neuralnet(fx, data = train, err.fct = "sse")
lrX<-lm(f,data = train)
lrX<-lm(nose_tip_x~.,data = train)
glm<-glm(as.formula(f),data = train)
lrX<-lm(fx,data = train)
glm<-glm(fx,data = train)
fy <- as.formula(paste("nose_tip_y ~", paste(n[!n %in% "nose_tip_y"], collapse = " + ")))
nnY<-neuralnet(fy, data = train, err.fct = "sse")
lrY<-lm(fy,data = train)
glmY<-glm(fy,data = train)
print(nnX)
write(print(nnX),"cout.txt")
sink("cout.txt")
print(nnX)
sink()
unlink("cout.txt")
plot(nnX)
pnnX<-compute(nnX,test)
testMX<-test
lrX<-lm(nose_tip_x~.,data = train)
glmX<-glm(nose_tip_x~.,data = train)
testMY<-test
lrY<-lm(nose_tip_y~.,data = train)
glmY<-glm(nose_tip_y~.,data = train)
plrX<-predict(lrX,data=test)
plrY<-predict(lrY,data=test)
pglrY<-predict(glrY,data=test)
glrX<-glm(nose_tip_x~.,data = train)
pglrX<-predict(glrX,data=test)
glrY<-glm(nose_tip_y~.,data = train)
pglrY<-predict(glrY,data=test)
testMX$nose_tip_x<-NULL
dtestM <- rbind(testMX,testMX)
dtestM<-data.frame(testMX,nose_tip_x=c(plrX,pglrX) )
plrX
pglrX
size(plrX)
nfil(plrX)
nrow(plrX)
ncol(plrX)
rm(list = ls())
library(randomForest)
library(pROC)
library(rpart)
library(gbm)
data <- read.csv("training/training.csv", stringsAsFactors = F)
data.update <- na.omit(data)
im.train <- data.update$Image
data.update$Image <- NULL
library("doParallel")
registerDoParallel()
im.train <- foreach(im = im.train, .combine=rbind) %dopar% {
as.integer(unlist(strsplit(im, " ")))
}
im.train <- as.data.frame(im.train)
data <- cbind(data.update, im.train)
ind <- sample(2, nrow(data), replace=TRUE, prob=c(0.8, 0.2))
train <- data.update[ind==1,]
test <- data.update[ind==2,]
##Stacking
#We will use linear regretion and general lineal regretion for the stacking, which
# later we use a SMV with this stacking.
library(e1071)
testMX<-test
lrX<-lm(nose_tip_x~.,data = train)
glrX<-glm(nose_tip_x~.,data = train)
plrX<-predict(lrX,data=test)
pglrX<-predict(glrX,data=test)
nrow(testMX)
nrow(train)
View(test)
testMX<-test
testMX$nose_tip_x<-NULL
plrX<-predict(lrX,data=testMX)
fit_2 <- lm(Volume ~ Girth + Height, data = trees)
Girth <- seq(9,21, by=0.5) ## make a girth vector
Height <- seq(60,90, by=0.5) ## make a height vector
pred_grid <- expand.grid(Girth = Girth, Height = Height)
View(pred_grid)
pred_grid$Volume2 <-predict(fit_2, new = pred_grid)
View(pred_grid)
plrX<-predict(lrX,new=testMX)
pglrX<-predict(glrX,new=testMX)
dtestM <- rbind(testMX,testMX)
dtestM<-data.frame(testMX,nose_tip_x=c(plrX,pglrX) )
419*2
testMX<-rbind(testMX,testMX)
testMX<-test
testMX$nose_tip_x<-NULL
testMX<-rbind(testMX,testMX)
rm(testMX)
testMX<-test
testMX$nose_tip_x<-NULL
testMX<-rbind(testMX,testMX)
testMX<-data.frame(testMX,nose_tip_x=c(plrX,pglrX) )
testMY<-test
lrY<-lm(nose_tip_y~.,data = train)
glrY<-glm(nose_tip_y~.,data = train)
plrY<-predict(lrY,data=test)
pglrY<-predict(glrY,data=test)
testMY<-test
testMY$nose_tip_y<-NULL
plrY<-predict(lrY,new=testMY)
pglrY<-predict(glrY,new=testMY)
testMY<-rbind(testMY,testMY)
testMY<-data.frame(testMY,nose_tip_Y=c(plrY,pglrY) )
StackSVM_X<-svm(nose_tip_x~.,data = testMX)
predStackx <- predict(StackX, test)
StackSVM_Y<-svm(nose_tip_y~.,data = testMY)
StackSVM_X<-svm(nose_tip_x~.,data = testMX)
StackSVM_Y<-svm(nose_tip_y~.,data = testMY)
testMY<-data.frame(testMY,nose_tip_y=c(plrY,pglrY) )
StackSVM_Y<-svm(nose_tip_y~.,data = testMY)
predStackx <- predict(StackSVM_X, test)
predStacky <- predict(StackSVM_y, test)
predStackx <- as.data.frame(predStackx)
predStacky <- as.data.frame(predStacky)
cmStack <- cbind(predStackx, truex = test$nose_tip_x)
cmStack <- cbind(cmStack, MSEx = ((cmStack$truex-cmStack$predStackx)^2))
cmStack <- cbind(cmStack,predStacky)
rm(list = ls())
library(randomForest)
library(pROC)
library(rpart)
library(gbm)
data <- read.csv("training/training.csv", stringsAsFactors = F)
data.update <- na.omit(data)
im.train <- data.update$Image
data.update$Image <- NULL
library("doParallel")
registerDoParallel()
im.train <- foreach(im = im.train, .combine=rbind) %dopar% {
as.integer(unlist(strsplit(im, " ")))
}
im.train <- as.data.frame(im.train)
data <- cbind(data.update, im.train)
ind <- sample(2, nrow(data), replace=TRUE, prob=c(0.8, 0.2))
train <- data.update[ind==1,]
test <- data.update[ind==2,]
##Stacking
#We will use linear regretion and general lineal regretion for the stacking, which
# later we use a SMV with this stacking.
library(e1071)
lrX<-lm(nose_tip_x~.,data = train)
glrX<-glm(nose_tip_x~.,data = train)
testMX<-test
testMX$nose_tip_x<-NULL
plrX<-predict(lrX,new=testMX)
pglrX<-predict(glrX,new=testMX)
testMX<-rbind(testMX,testMX)
testMX<-data.frame(testMX,nose_tip_x=c(plrX,pglrX) )
testMY<-test
lrY<-lm(nose_tip_y~.,data = train)
glrY<-glm(nose_tip_y~.,data = train)
plrY<-predict(lrY,data=test)
pglrY<-predict(glrY,data=test)
testMY<-test
testMY$nose_tip_y<-NULL
plrY<-predict(lrY,new=testMY)
pglrY<-predict(glrY,new=testMY)
testMY<-rbind(testMY,testMY)
testMY<-data.frame(testMY,nose_tip_y=c(plrY,pglrY) )
StackSVM_X<-svm(nose_tip_x~.,data = testMX)
StackSVM_Y<-svm(nose_tip_y~.,data = testMY)
predStackx <- predict(StackSVM_X, test)
predStacky <- predict(StackSVM_y, test)
predStacky <- predict(StackSVM_Y, test)
predStackx <- as.data.frame(predStackx)
predStacky <- as.data.frame(predStacky)
cmStack <- cbind(predStackx, truex = test$nose_tip_x)
cmStack <- cbind(cmStack, MSEx = ((cmStack$truex-cmStack$predStackx)^2))
cmStack <- cbind(cmStack,predStacky)
cmStack <- cbind(cmStack, truey = test$nose_tip_y)
cmStack <- cbind(cmStack, MSEy = ((cmStack$truey-cmStack$predStacky)^2))
MSE_MEAN_STACKx <- mean(cmStack$MSEx)
MSE_MEAN_STACKy <- mean(cmStack$MSEy)
cat("MSE for the nose_tip_x for Adaboost: ",MSE_MEAN_STACKx,"\n")
cat("MSE for the nose_tip_y for Adaboost: ",MSE_MEAN_STACKy,"\n")
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 3/Face Detection/facialdetectionEmsemblers.R')
MSE_MEAN_RFx <- mean(cm_rf$MSEx)
MSE_MEAN_RFy <- mean(cm_rf$MSEy)
View(cm_rf)
cm_rf <- cbind(cm_rf, MSEy = ((cm$truey-cm$predRFy)^2))
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 3/Face Detection/facialdetectionEmsemblers.R')
