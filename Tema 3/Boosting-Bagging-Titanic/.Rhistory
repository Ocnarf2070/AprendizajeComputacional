data <- read.csv("training/training.csv", stringsAsFactors = F)
data <- read.csv("training/training.csv", stringsAsFactors = F)
corregido <- na.omit(data)
im.train <- corregido$Image
corregido$Image <- NULL
library("doParallel")
registerDoParallel()
im.train <- foreach(im = im.train, .combine=rbind) %dopar% {
as.integer(unlist(strsplit(im, " ")))
}
im.train <- as.data.frame(im.train)
data <- cbind(corregido, im.train)
ind <- sample(2, nrow(data), replace=TRUE, prob=c(0.8, 0.2))
train <- corregido[ind==1,]
test <- corregido[ind==2,]
arbolx <- randomForest(nose_tip_x~., train)
arboly <- randomForest(nose_tip_y~., train)
importance(arbolx)
importance(arboly)
predRFx <- predict(arbolx, test)
predRFx <- as.data.frame(predRFx)
predRFy <- predict(arboly, test)
predRFy <- as.data.frame(predRFy)
cm <- cbind(predRFx, truex = test$nose_tip_x)
cm <- cbind(cm, MAPEx = (abs(cm$predRFx-cm$truex)/cm$truex)*100)
cm <- cbind(cm, predRFy)
cm <- cbind(cm, truey = test$nose_tip_y)
cm <- cbind(cm, MAPEy = (abs(cm$predRFy-cm$truey)/cm$truey)*100)
MAPE_PROMEDIORFX <- sum(cm$MAPEx)/nrow(cm)
MAPE_PROMEDIORFY <- sum(cm$MAPEy)/nrow(cm)
MAPE_PROMEDIORFX
MAPE_PROMEDIORFY
adaX <- gbm(nose_tip_x~., data=train, distribution="gaussian", cv.folds = 3) # Al ser regresion el paquete adaboost no sirve
adaY <- gbm(nose_tip_y~., data=train, distribution="gaussian", cv.folds = 3) # hay que usar gbm (adaboost para regresion)
best.iterX <- gbm.perf(adaX,method="cv")
best.iterY <- gbm.perf(adaY,method="cv")
predAdax <- predict(adaX, test, best.iterX)
predAday <- predict(adaY, test, best.iterY)
predAdax <- as.data.frame(predAdax)
predAday <- as.data.frame(predAday)
cmAda <- cbind(predAdax, truex = test$nose_tip_x)
cmAda <- cbind(cmAda, MAPEx = (abs(cmAda$predAdax-cmAda$truex)/cmAda$truex)*100)
cmAda <- cbind(cmAda,predAday)
cmAda <- cbind(cmAda, truey = test$nose_tip_y)
cmAda <- cbind(cmAda, MAPEy = (abs(cmAda$predAday-cmAda$truey)/cmAda$truey)*100)
MAPE_PROMEDIOADAX <- sum(cmAda$MAPEx)/nrow(cmAda)
MAPE_PROMEDIOADAY <- sum(cmAda$MAPEy)/nrow(cmAda)
MAPE_PROMEDIOADAX
MAPE_PROMEDIOADAY
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 3/Face Detection/facialdetectionRamdomForest.R')
rm(list = ls())
x1=rnorm(1000)
x2=rnorm(1000)
y=2*x1+.7*x2+rnorm(1000)
df=data.frame(y,x1,x2,x3=rnorm(1000),x4=rnorm(1000),x5=rnorm(1000)
)
# run the randomForest implementation
library(randomForest)
rf1 <- randomForest(y~., data=df, mtry=2, ntree=50,
importance=TRUE)
importance(rf1,type=1)
importance(rf1,type=2)
importance(rf1,type=1)
rf1 <- randomForest(y~., data=df, mtry=2, ntree=50,
importance=TRUE)
importance(rf1,type=1)
rf1 <- randomForest(y~., data=df, mtry=2, ntree=50,
importance=TRUE)
importance(rf1,type=1)
rf1 <- randomForest(y~., data=df, mtry=2, ntree=50,
importance=FALSE)
importance(rf1,type=1)
TRUE
importance(rf1,type=1)
rf1 <- randomForest(y~., data=df, mtry=2, ntree=50,
importance=TRUE)
importance(rf1,type=1)
importance(rf1)
rf1 <- randomForest(y~., data=df, mtry=2, ntree=50,
importance=F)
importance(rf1)
rf1 <- randomForest(y~., data=df, mtry=2, ntree=50,
importance=T)
importance(rf1)
sum(importance((rf1,type=1)))
sum(importance(rf1,type=1))
plot(rf1)
plot(as.tree(rf1))
library(reprtree)
install.packages(reprtree)
plot(df)
datos <- read.csv("train.csv", header=T, quote="\"")
adadata<- datos
adadata$Survived  <- factor(adadata$Survived)
d_size <- dim(adadata)[1]
dtest_size <- ceiling(0.2*d_size)
set.seed(200)
samples <- sample(d_size, d_size, replace=FALSE)
indexes <- samples[1:dtest_size]
dtrain <- adadata[-indexes,]
dtest <- adadata[indexes,]
setwd("D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 3/Boosting-Bagging-Titanic")
datos <- read.csv("train.csv", header=T, quote="\"")
adadata<- datos
adadata$Survived  <- factor(adadata$Survived)
d_size <- dim(adadata)[1]
dtest_size <- ceiling(0.2*d_size)
set.seed(200)
samples <- sample(d_size, d_size, replace=FALSE)
indexes <- samples[1:dtest_size]
dtrain <- adadata[-indexes,]
dtest <- adadata[indexes,]
formulae<-Survived~PassengerId+Pclass+Sex+Age+SibSp+Ticket+Fare+Cabin+Embarked
rt<-randomForest(formulae,data=dtrain,mtry=2,ntree=50,importance=T)
rt<-randomForest(formulae,data=dtrain,mtry=2,ntree=50,importance=T,na.action = na.omit)
rt<-randomForest(formulae,data=dtrain,mtry=2,ntree=100,importance=T,na.action = na.omit)
rt<-randomForest(formulae,data=dtrain,ntree=100,importance=T,na.action = na.omit)
rm(list = ls())
x1=rnorm(1000)
x2=rnorm(1000)
y=2*x1+.7*x2+rnorm(1000)
df=data.frame(y,x1,x2,x3=rnorm(1000),x4=rnorm(1000),x5=rnorm(1000)
)
# run the randomForest implementation
library(randomForest)
rf1 <- randomForest(y~., data=df, mtry=2, ntree=50,
importance=T)
importance(rf1)
hist(importance(rf1))
hist(importance(rf1,type = 1))
rf1 <- randomForest(y~., data=df, mtry=2, ntree=100,
importance=T)
importance(rf1,type = 1)
library(randomForest)
library(pROC)
library(rpart)
library(adabag)
library(gbm)
data <- read.csv("training/training.csv", stringsAsFactors = F)
corregido <- na.omit(data)
setwd("D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 3/Face Detection")
data <- read.csv("training/training.csv", stringsAsFactors = F)
corregido <- na.omit(data)
im.train <- corregido$Image
corregido$Image <- NULL
library("doParallel")
registerDoParallel()
im.train <- foreach(im = im.train, .combine=rbind) %dopar% {
as.integer(unlist(strsplit(im, " ")))
}
im.train <- as.data.frame(im.train)
data <- cbind(corregido, im.train)
ind <- sample(2, nrow(data), replace=TRUE, prob=c(0.8, 0.2))
train <- corregido[ind==1,]
test <- corregido[ind==2,]
arbolx <- randomForest(nose_tip_x~., train)
arboly <- randomForest(nose_tip_y~., train)
importance(arbolx)
importance(arboly)
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 3/Bagging Boosting con Caret.R')
install.packages("mlbench")
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 3/Bagging Boosting con Caret.R')
install.packages("caretEnsemble")
install.packages("caret")
install.packages("caret")
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 3/Bagging Boosting con Caret.R')
rm(list = ls())
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 3/Stacking con SVM CARET.R')
dotplot(results)
summary(results)
print(stackingSVM)
confusionMatrix(prediccionGlobal,dtest$Species )
prediccionGlobal<-predict(stackingSVM,dtest,type="raw")
confusionMatrix(prediccionGlobal,dtest$Species )
ind <- sample(150,150)
idt <- ind[1:10]
dtrain <- iris[-idt,]
dtest <- iris[idt,]
dtrain$Species=as.numeric(dtrain$Species)
dtest$Species=as.numeric(dtest$Species)
# Example of Stacking algorithms
# create submodels
control <- trainControl(method="repeatedcv", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)
algorithmList <- c('nnet', 'svmRadial')
set.seed(seed)
models <- caretList(Species~., data=dtrain, trControl=control, methodList=algorithmList)
results <- resamples(models)
summary(results)
dotplot(results)
# stack using random forest
set.seed(seed)
stackControl <- trainControl(method="repeatedcv", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)
stackingSVM <- caretStack(models, method="svmRadial",  trControl=stackControl)
print(stackingSVM)
prediccionGlobal<-predict(stackingSVM,dtest,type="raw")
confusionMatrix(prediccionGlobal,dtest$Species )
prediccionGlobal
dtest$Species
length(dtest$Species)
length(prediccionGlobal)
models <- caretList(as.factor(Species)~., data=dtrain, trControl=control, methodList=algorithmList)
results <- resamples(models)
models <- caretList(as.factor(Species)~., data=dtrain, trControl=control, methodList=algorithmList)
models <- caretList(Species~., data=dtrain, trControl=control, methodList=algorithmList)
confusionMatrix(prediccionGlobal )
confusionMatrix(table(prediccionGlobal,dtest$Species ))
confusionMatrix(prediccionGlobal,sample(dtest$Species ))
confusionMatrix(iris$Species,sample(dtest$Species ))
xtab <- confusionMatrix(iris$Species, sample(iris$Species))
as.matrix(xtab)
xtab <- confusionMatrix(iris$Species,sample(dtest$Species ))
dtest
prediccionGlobal
dtrain
table(prediccionGlobal,sample(dtest$Species ))
table(prediccionGlobal,dtest$Species )
prediccionGlobal
dtest$Species
confusionMatrix(round(prediccionGlobal),sample(dtest$Species ))
table(round(prediccionGlobal),sample(dtest$Species ))
round(prediccionGlobal)
dtest$Species
confusionMatrix(factor(prediccionGlobal),factor(dtest$Species ))
confusionMatrix(factor(round(prediccionGlobal)),factor(dtest$Species ))
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 3/Stacking con SVM CARET.R')
confusionMatrix(factor(round(prediccionGlobal)),factor(dtest$Species ))
rm(list = ls())
library(rpart)
library(nnet)
library(e1071)
ind <- sample(150,150)
idt <- ind[1:10]
dtrain <- iris[-idt,]
dtest <- iris[idt,]
dtestM<-dtest
perceptron<-nnet(Species~., dtrain, size=2)
arbol<-rpart(Species~., dtrain)
pper<-predict(perceptron,dtest,type="class")
par<-predict(arbol,dtest,type="class")
par
dtestM$Species<-NULL
dtestM <- rbind(dtestM,dtestM)
dtestM<-data.frame(dtestM,Species=c(as.character(par) ,pper) )
dtestM
fitSVM<-svm(Species~., dtestM )
rm(list = ls())
ind <- sample(150,150)
idt <- ind[1:10]
dtrain <- iris[-idt,]
dtest <- iris[idt,]
dtestM<-dtest
perceptron<-nnet(Species~., dtrain, size=2)
arbol<-rpart(Species~., dtrain)
pper<-predict(perceptron,dtest,type="class")
par<-predict(arbol,dtest,type="class")
pper
par
perceptron<-nnet(Species~., dtrain, size=3)
pper<-predict(perceptron,dtest,type="class")
pper
par
dtestM
dtestM
dtestM$Species<-NULL
dtestM
dtestM <- rbind(dtestM,dtestM)
dtestM
dtestM<-data.frame(dtestM,Species=c(as.character(par) ,pper) )
dtestM
fitSVM<-svm(Species~., dtestM )
prediccionGlobal<-predict(fitSVM,dtest,type="class")
matrizconfusion<-table(prediccionGlobal, dtest$Species)
matrizconfusion
accuracy<-sum (diag(matrizconfusion))/sum (matrizconfusion)
accuracy
rm(list = ls())
library(rpart)
datos<- read.csv("datos_icb.txt", sep="")
setwd("D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 3")
datos<- read.csv("datos_icb.txt", sep="")
# Generamos ÃƒÂ¡rboles sobre el dataset
inicio = 0.6
limite = 0.95
pasos = 6
incremento = (limite - inicio)/pasos
tamTrain=0.8
clasificadores = c();
for(i in 0:5){
set.seed(15*i);
ind<- sample(500,500)
idt <- ind[1:25]
idt
dtrain <- datos[-idt,]
dtest <- datos[idt,]
clasificadores[i] <- rpart(recid ~., data = dtrain, control = rpart.control(minsplit = 10))
clasificadores[i] <- c(clasificadores, list(cl))
}
for(i in 0:5){
set.seed(15*i);
ind<- sample(500,500)
idt <- ind[1:25]
idt
dtrain <- datos[-idt,]
dtest <- datos[idt,]
clasificadores[i] <- rpart(recid ~., data = dtrain, control = rpart.control(minsplit = 10))
clasificadores[i] <- c(clasificadores, list(clasificadores))
}
clasificadores = c();
for(i in 0:5){
set.seed(15*i);
ind<- sample(500,500)
idt <- ind[1:25]
idt
dtrain <- datos[-idt,]
dtest <- datos[idt,]
clasificadores[i,] <- rpart(recid ~., data = dtrain, control = rpart.control(minsplit = 10))
clasificadores[i,] <- c(clasificadores, list(clasificadores))
}
d<-rep(1,10)
d<-d*1/10
n<-readline()
n
n<-readline(prompt = "How many observations have?\n")
d<-rep(1,n)*1/n;
d<-rep(1,n);
d<-d*1/n
1/n
n<-as.numeric(readline(prompt = "How many observations have?\n"))
d<-rep(1,n)*1/n;
cat(d)
index <- c(1,3,5)
d[index]
index <- as.numeric(readline(prompt = "What observations are wrong?"))
index <- readline(prompt = "What observations are wrong?")
d[index]
as.list.numeric_version(index)
m<-as.list.numeric_version(index)
d[m]
m<-as.numeric_version(index)
m<-as.numeric(index)
m<-as.integer(index)
index[1]
c(index)
c(paste(index,collapse = ","))
paste(index,collapse = ",")
index[[]]
index[[1]]
p
index <- readline(prompt = "What observations are wrong?")
type.convert(index)
m<-type.convert(index)
m<-type.convert(index,as.is = "integer")
m
m<-type.convert(index,numerals = "no.loss")
m
index <- readline(prompt = "What observations are wrong?")
m<-type.convert(index,numerals = "no.loss")
m
m<-type.convert(index,as.is = T)
m
strtoi(index)
strtoi(c("1","2"))
m<-c("1","2")
strsplit(index," ")
m<-strsplit(index," ")
strtoi(index)
m[[1]]
strtoi(m)
strtoi(m[[]])
strtoi(m[[1]])
mn<-strtoi(m[[1]])
index <- strtoi(strsplit(readline(prompt = "What observations are wrong?")," "))
index <- strsplit(readline(prompt = "What observations are wrong?")," "))
index <- strsplit(readline(prompt = "What observations are wrong?")," ")
index <- strtoi(index[[1]])
d[index]
cat("\alpha")
cat("\(\alpha\)")
cat(expression(alpha))
expression(alpha)
cat(sprintf("d%d: %.4f\n",i,d))
cat(sprintf("d%d: %.4f\n",i,paste(d,collapse = ", "))
index <- strsplit(readline(prompt = "What observations are wrong?")," ")
index <- strtoi(index[[1]])
error <- sum(d[index])
cat(sprintf("error%d: %.4f\n",i,error));
alpha=(1/2)*((1-error)/error);
cat(sprintf("alpha%d: %.4f\n",i,alpha));
}
cat(sprintf("d%d: %.4f\n",i,paste(d,collapse = ", ")))
cat(sprintf("d%d: %s\n",i,paste(d,collapse = ", ")))
y_h<-rep(1,n)
y_h[index]=-1
index <- strsplit(readline(prompt = "What observations are wrong?")," ")
index <- strtoi(index[[1]])
y_h[index]=-1
y_h
cat(sprintf("d%d: %s",i,paste(d,collapse = "\t ")))
error <- sum(d[index])
cat(sprintf(" error%d: %.4f",i,error));
alpha=(1/2)*((1-error)/error);
cat(sprintf(" alpha%d: %.4f\n",i,alpha));
e<-exp(-alpha*y_h)
cat(sprintf("e:  %s",i,paste(d,collapse = "\t ")))
-alpha/y_h
-alpha*y_h
exp(-alpha*y_h)
cat(sprintf("e:  %s",i,paste(e,collapse = "\t ")))
e<-exp(-alpha*y_h)
cat(sprintf("e:  %s",i,paste(e,collapse = "\t ")))
paste(e,collapse = "\t ")
cat(sprintf("e:  %s",i,paste(e,collapse = " ")))
cat(sprintf("e:  %.4f",e)
}
cat(sprintf("e:  %.4f",e))
cat(sprintf("%.4f",e))
source('~/.active-rstudio-document')
cat(sprintf("%.4f\t",e))
index<-c(1,2,3)
error <- sum(d[index])
alpha=(1/2)*((1-error)/error);
e<-exp(-alpha*y_h)
y_h[index]=-1
e<-exp(-alpha*y_h)
alpha=(1/2)*log((1-error)/error);
e<-exp(-alpha*y_h)
d*e
source('~/.active-rstudio-document')
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 3/ADABOOSTING(CALCULO).R')
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 3/ADABOOSTING(CALCULO).R')
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 3/ADABOOSTING(CALCULO).R')
rm(list = ls())
x1=rnorm(1000)
x2=rnorm(1000)
y=2*x1+.7*x2+rnorm(1000)
df=data.frame(y,x1,x2,x3=rnorm(1000),x4=rnorm(1000),x5=rnorm(1000)
)
# run the randomForest implementation
library(randomForest)
rf1 <- randomForest(y~., data=df, mtry=2, ntree=100,
importance=T)
importance(rf1,type = 1)
setwd("D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 3/Boosting-Bagging-Titanic")
rm(list = ls())
library(adabag);
datos <- read.csv("train.csv", header=T, quote="\"")
adadata<- datos
adadata$Survived  <- factor(adadata$Survived)
d_size <- dim(adadata)[1]
dtest_size <- ceiling(0.2*d_size)
set.seed(200)
samples <- sample(d_size, d_size, replace=FALSE)
indexes <- samples[1:dtest_size]
dtrain <- adadata[-indexes,]
dtest <- adadata[indexes,]
adaboost <- boosting(Survived~PassengerId+Pclass+Sex+Age+SibSp+Ticket+Fare+Cabin+Embarked, data=dtrain, boos=TRUE, mfinal=10,coeflearn='Breiman')
summary(adaboost)
adaboost$trees
adaboost$weights
adaboost$importance
errorevol(adaboost,adadata)
predict(adaboost,adadata)
tree1 <- adaboost$trees[[1]]
library(rattle)
library(rpart.plot)
library(RColorBrewer)
plot(tree1)
plot(tree1)
text(tree1)
errorevol(adaboost,adadata)
predict(adaboost,adadata)
pred<-predict(adaboost,adadata)
pred
prediction <- predict (object = tree1, newdata = dtest, type="class")
prediction
MC <-table(dtest[, "Survived"],prediction);MC
TruePos <-MC[1,2]/(MC[1,1]+MC[1,2]);TruePos
Scores <- sum(diag(MC))
Accuracy <- Scores/(sum(MC));Accuracy
rm(list = ls())
x1=rnorm(1000)
x2=rnorm(1000)
y=2*x1+.7*x2+rnorm(1000)
df=data.frame(y,x1,x2,x3=rnorm(1000),x4=rnorm(1000),x5=rnorm(1000)
)
# run the randomForest implementation
library(randomForest)
rf1 <- randomForest(y~., data=df, mtry=2, ntree=100,
importance=T)
importance(rf1,type = 1)
plot(rf1)
View(rf1)
importanceplot(rf1)
importanceSD(rf1)
rf1$importance
importance(rf1)
rf1$importanceSD
rf1$importance[1,]
rf1$importance[,1]
rf1$importance[,1]*rf1$importanceSD
rf1$importance[,1]/rf1$importanceSD
importanceSD(rf1,type=1)
importance(rf1,type=1)
histogram(importance(rf1,type=1))
histogram(1:5,importance(rf1,type=1))
hist(1:5,importance(rf1,type=1))
as.matrix(importance(rf1,type=1))
histogram(as.matrix(importance(rf1,type=1)))
hist(as.matrix(importance(rf1,type=1)))
barchart(as.matrix(importance(rf1,type=1)))
barchart(importance(rf1,type=1))
barchart(importance(rf1,type=1))
