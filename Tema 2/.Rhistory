x.svm <-x [svm.lineal1$index,]
w <- crossprod(x.svm, svm.lineal1$coefs)
w0 <- svm.lineal1$rho
#Vectores Soporte
show(summary(svm.lineal1))
rm(list = ls())
#c)--------------------------------------------------
x1<-c(2,2,-2,-2,2,2,-2,-2,1,1,-1,-1)
x2<-c(2,-2,-2,2,2,-2,-2,2,1,-1,-1,1)
y<-c(1,1,1,1,1,1,1,1,-1,-1,-1,-1)
Data3<-data.frame(x1,x2,y)
x<-cbind(Data3$x1, Data3$x2)
y <- Data3$y
n0 <- sum(y=='1')
n1 <- sum(y=='-1')
colores <- c(rep("green",n0), rep("red",n1))
pchn <- 21
# Diagrama de dispersion
plot(x, pch = pchn, bg = colores)
#Formula, SVM y datos importantes
formula<-as.factor(y)~.
#Como no es lineal, utilizaremos otro kernel
svm.pol <- svm (formula, data=Data3,kernel = "radial", cost = 1000, scale = FALSE)
#Vectores Soporte
show(summary(svm.pol))
svm.model <- svm(Species ~ ., data = iris)
summary(svm.model)
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 2/Titanic/Titanic.R')
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 2/Titanic/Titanic.R')
predictions <- round(predict(svm1, DataTest[1:100,]))
DataTest[1:100,]
DataTest[,1:100]
DataTest[1:100,]
debugSource('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 2/Titanic/Titanic.R')
summaty(svm)
summary(svm)
svm <- svm(as.factor(Survived)~ ., dtrain,kernel='radial')
summary(svm)
predictions <- round(predict(svm, DataTest[1:100,]))
debugSource('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 2/Titanic/Titanic.R')
debugSource('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 2/Titanic/Titanic.R')
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 2/Titanic/Titanic.R')
plot(svm1,DataTrain)
# The best one is the SVM. We train it now with all our data:
svm <- svm(as.Factor(Survived) ~ ., DataTrain)
# The best one is the SVM. We train it now with all our data:
svm <- svm(as.factor(Survived) ~ ., DataTrain)
predictions <- round(predict(svm, DataTest[1:100,]))
plot(svm,DataTrain)
# The best one is the SVM. We train it now with all our data:
svm <- svm(Survived ~ . -Survived, DataTrain)
predictions <- round(predict(svm, DataTest[,-1]))
predictions <- round(predict(svm, DataTest))
# The best one is the SVM. We train it now with all our data:
svm <- svm(Survived ~ ., DataTrain)
predictions <- round(predict(svm, DataTest[,-1]))
predictions <- round(predict(svm, DataTest,type="response"))
#f)--------------------------------------------------
data(iris)
x <- subset(iris, select=-Species)
y <- iris$Species
n0 <- sum(y=='setosa')
n1 <- sum(y=='versicolor')
n2 <- sum(y=='virginica')
colores <- c(rep("green",n0), rep("red",n1),rep("purple",n2))
pchn <- 21
svm.lineal5 <- svm (x,y,kernel = "linear")
# Diagrama de dispersion
plot(x, pch = pchn, bg = colores)
x.svm <-x [svm.lineal5$index,]
w <- crossprod(x.svm, svm.lineal5$coefs)
svm.lineal5$coefs
x.svm*x.svm
x.svm[1]
#c)--------------------------------------------------
x1<-c(2,2,-2,-2,2,2,-2,-2,1,1,-1,-1)
x2<-c(2,-2,-2,2,2,-2,-2,2,1,-1,-1,1)
y<-c(1,1,1,1,1,1,1,1,-1,-1,-1,-1)
Data3<-data.frame(x1,x2,y)
x<-cbind(Data3$x1, Data3$x2)
y <- Data3$y
n0 <- sum(y=='1')
n1 <- sum(y=='-1')
colores <- c(rep("green",n0), rep("red",n1))
pchn <- 21
# Diagrama de dispersion
plot(x, pch = pchn, bg = colores)
#Formula, SVM y datos importantes
formula<-as.factor(y)~.
#Como no es lineal, utilizaremos otro kernel
svm.pol <- svm (formula, data=Data3,kernel = "radial", cost = 1000, scale = FALSE)
x.svm <-x [svm.pol$index,]
svm.pol$index
size(x.svm)
length(x.svm)
nrow(x.svm)
#Vectores Soporte
cat("Vectores Soporte (W):\n")
for(i in 1:nrow(x.svm)){
text<-sprintf("(%s)\n",paste(x.svm[i], collapse=", "))
cat(text)
}
for(i in 1:nrow(x.svm)){
text<-sprintf("(%s)\n",paste(x.svm[i,], collapse=", "))
cat(text)
}
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 2/SMV/SVM(1).R')
#e)--------------------------------------------------
x1<-c(3,3,6,6,1,0,0,-1)
x2<-c(1,-1,1,-1,0,1,-1,0)
y<-c(1,1,1,1,-1,-1,-1,-1)
Data5<-data.frame(x1,x2,y)
x<-cbind(Data5$x1, Data5$x2)
y <- Data5$y
n0 <- sum(y=='1')
n1 <- sum(y=='-1')
colores <- c(rep("green",n0), rep("red",n1))
pchn <- 21
# Diagrama de dispersion
plot(x, pch = pchn, bg = colores)
#Formula, SVM y datos importantes
formula<-as.factor(y)~.
svm.lineal4 <- svm (formula, data=Data5,kernel = "linear", cost = 1000, scale = FALSE)
x.svm <-x [svm.lineal4$index,]
w <- crossprod(x.svm, svm.lineal4$coefs)
w0 <- svm.lineal4$rho
#Vectores Soporte
cat("Vectores Soporte (W):\n")
for(i in 1:nrow(x.svm)){
text<-sprintf("(%s)\n",paste(x.svm[i,], collapse=", "))
cat(text)
}
#Valores del kernel
cat("Valores del kernel\n")
x.svm*x.svm
A<-x[svm.lineal4$index[1],]
B<-x[svm.lineal4$index[2],]
text<-sprintf("K(A,A)=%.2f\n",crossprod(A,A))
cat(text)
text<-sprintf("K(A,B)=%.2f\n",crossprod(A,B))
cat(text)
text<-sprintf("K(B,A)=%.2f\n",crossprod(B,A))
cat(text)
text<-sprintf("K(B,B)=%.2f\n",crossprod(B,B))
cat(text)
crossprod(x.svm,x.svm)
C<-x[svm.lineal4$index[3],]
text<-sprintf("K(A,A)=%.2f\n",crossprod(A,A))
cat(text)
text<-sprintf("K(A,B)=%.2f\n",crossprod(A,B))
cat(text)
text<-sprintf("K(A,C)=%.2f\n",crossprod(A,C))
cat(text)
text<-sprintf("K(B,A)=%.2f\n",crossprod(B,A))
cat(text)
text<-sprintf("K(B,B)=%.2f\n",crossprod(B,B))
cat(text)
text<-sprintf("K(B,C)=%.2f\n",crossprod(B,C))
cat(text)
text<-sprintf("K(C,A)=%.2f\n",crossprod(C,A))
cat(text)
text<-sprintf("K(C,B)=%.2f\n",crossprod(C,B))
cat(text)
text<-sprintf("K(C,C)=%.2f\n",crossprod(C,C))
cat(text)
x.svm*x.svm
#a)--------------------------------------------------
x1<-c(0,4)
x2<-c(0,4)
y<-c(1,-1)
Data1<-data.frame(x1,x2,y)
x<-cbind(Data1$x1, Data1$x2)
y <- Data1$y
n0 <- sum(y=='1')
n1 <- sum(y=='-1')
colores <- c(rep("green",n0), rep("red",n1))
pchn <- 21
# Diagrama de dispersion
plot(x, pch = pchn, bg = colores)
#Formula, SVM y datos importantes
formula<-as.factor(y)~.
svm.lineal1 <- svm (formula, data=Data1,kernel = "linear", cost = 1000, scale = FALSE)
x.svm <-x [svm.lineal1$index,]
w <- crossprod(x.svm, svm.lineal1$coefs)
w0 <- svm.lineal1$rho
#Vectores Soporte
cat("Vectores Soporte (W):\n")
text<-sprintf("(%s)\n",paste(x.svm[1,], collapse=", "))
cat(text)
text<-sprintf("(%s)\n",paste(x.svm[2,], collapse=", "))
cat(text)
#Valores del kernel
cat("Valores del kernel\n")
A<-x[svm.lineal1$index[1],]
B<-x[svm.lineal1$index[2],]
text<-sprintf("K(A,A)=%.2f\n",crossprod(A,A))
cat(text)
text<-sprintf("K(A,B)=%.2f\n",crossprod(A,B))
cat(text)
text<-sprintf("K(B,A)=%.2f\n",crossprod(B,A))
cat(text)
text<-sprintf("K(B,B)=%.2f\n",crossprod(B,B))
cat(text)
#Ancho de banda
text<-sprintf("Ancho de banda: %.5f\n",2/norm(w,type = '2'))
x.svm*x.svm
x.svm%*%x.svm
#f)--------------------------------------------------
data(iris)
x <- subset(iris, select=-Species)
y <- iris$Species
n0 <- sum(y=='setosa')
n1 <- sum(y=='versicolor')
n2 <- sum(y=='virginica')
colores <- c(rep("green",n0), rep("red",n1),rep("purple",n2))
pchn <- 21
# Diagrama de dispersion
plot(x, pch = pchn, bg = colores)
#Formula, SVM y datos importantes
svm.lineal5 <- svm (x,y,kernel = "linear")
x.svm <-x [svm.lineal5$index,]
w <- crossprod(as.matrix(x.svm),as.matrix(svm.lineal5$coefs))
w0 <- svm.lineal5$rho
#Vectores Soporte
for(i in 1:nrow(x.svm)){
text<-sprintf("(%s)\n",paste(x.svm[i,], collapse=", "))
cat(text)
}
#Valores del kernel
cat("Valores del kernel\n")
A<-x[svm.lineal5$index[1],]
B<-x[svm.lineal5$index[2],]
text<-sprintf("K(A,A)=%.2f\n",crossprod(A,A))
cat(text)
text<-sprintf("K(A,B)=%.2f\n",crossprod(A,B))
cat(text)
A<-as.matrix(x[svm.lineal5$index[1],])
B<-as.matrix(x[svm.lineal5$index[2],])
text<-sprintf("K(A,A)=%.2f\n",crossprod(A,A))
cat(text)
text<-sprintf("K(A,B)=%.2f\n",crossprod(A,B))
cat(text)
text<-sprintf("K(B,A)=%.2f\n",crossprod(B,A))
cat(text)
text<-sprintf("K(B,B)=%.2f\n",crossprod(B,B))
cat(text)
A<-as.matrix(x.svm[1,])
B<-as.matrix(x.svm[2,])
text<-sprintf("K(A,A)=%.2f\n",crossprod(A,A))
cat(text)
text<-sprintf("K(A,B)=%.2f\n",crossprod(A,B))
cat(text)
text<-sprintf("K(B,A)=%.2f\n",crossprod(B,A))
cat(text)
text<-sprintf("K(B,B)=%.2f\n",crossprod(B,B))
cat(text)
#Ancho de banda
text<-sprintf("Ancho de banda: %.5f\n",2/norm(w,type = '2'))
cat(text)
#Vector de pesos normal al hiperplano
text<-sprintf("Vector de pesos normal al hiperplano (W): (%s)\n",paste(w, collapse=", "))
cat(text)
# Termino independiente
text<-sprintf("Termino independiente (B): %.2f\n",w0)
cat(text)
#Ecuaciones de hiperplano
text<-sprintf("Hiperplanos:\n%.5fx+%.5fy+%.2f=0\n",w[1],w[2],-w0)
cat(text)
text<-sprintf("%.5fx+%.5fy+%.2f=-1\n",w[1],w[2],-w0)
cat(text)
text<-sprintf("%.5fx+%.5fy+%.2f=1\n",w[1],w[2],-w0)
cat(text)
lim<-x+c(-2,2)
plot(x, pch = pchn, bg = colores,xlim = c(min(lim[,1]), max(lim[,1])), ylim =c(min(lim[,2]), max(lim[,2])) )
abline(w0/w[2], -w[1]/w[2])
abline((w0 - 1) / w[2], -w[1] / w[2], lty = 2)
abline((w0 + 1) / w[2], -w[1] / w[2], lty = 2)
#plot(x, pch = pchn, bg = colores,xlim = c(min(lim[,1]), max(lim[,1])), ylim =c(min(lim[,2]), max(lim[,2])) )
plot(x$Petal.Length, x$Petal.Width, pch = pchn, bg = colores)
abline(w0/w[2], -w[1]/w[2])
abline((w0 - 1) / w[2], -w[1] / w[2], lty = 2)
abline((w0 + 1) / w[2], -w[1] / w[2], lty = 2)
show( f.svm*f.svm )
show( x.svm*x.svm )
#c)--------------------------------------------------
x1<-c(2,2,-2,-2,2,2,-2,-2,1,1,-1,-1)
x2<-c(2,-2,-2,2,2,-2,-2,2,1,-1,-1,1)
y<-c(1,1,1,1,1,1,1,1,-1,-1,-1,-1)
Data3<-data.frame(x1,x2,y)
x<-cbind(Data3$x1, Data3$x2)
y <- Data3$y
n0 <- sum(y=='1')
n1 <- sum(y=='-1')
colores <- c(rep("green",n0), rep("red",n1))
pchn <- 21
# Diagrama de dispersion
plot(x, pch = pchn, bg = colores)
#Formula, SVM y datos importantes
formula<-as.factor(y)~.
#Como no es lineal, utilizaremos otro kernel
svm.pol <- svm (formula, data=Data3,kernel = "radial", cost = 1000, scale = FALSE)
x.svm <-x [svm.pol$index,]
#Vectores Soporte
cat("Vectores Soporte (W):\n")
for(i in 1:nrow(x.svm)){
text<-sprintf("(%s)\n",paste(x.svm[i,], collapse=", "))
cat(text)
}
#Valores del kernel
cat("Valores del kernel\n")
show( x.svm*x.svm )
t(x.svm)
show( x.svm*t(x.svm) )
show( x.svm%*%t(x.svm) )
#a)--------------------------------------------------
x1<-c(0,4)
x2<-c(0,4)
y<-c(1,-1)
Data1<-data.frame(x1,x2,y)
x<-cbind(Data1$x1, Data1$x2)
y <- Data1$y
n0 <- sum(y=='1')
n1 <- sum(y=='-1')
colores <- c(rep("green",n0), rep("red",n1))
pchn <- 21
# Diagrama de dispersion
plot(x, pch = pchn, bg = colores)
#Formula, SVM y datos importantes
formula<-as.factor(y)~.
svm.lineal1 <- svm (formula, data=Data1,kernel = "linear", cost = 1000, scale = FALSE)
x.svm <-x [svm.lineal1$index,]
w <- crossprod(x.svm, svm.lineal1$coefs)
w0 <- svm.lineal1$rho
show( x.svm%*%t(x.svm) )
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 2/SMV/SVM(1).R')
show( xs%*%t(xs) )
xs<-as.matrix(x.svm)
show( xs%*%t(xs) )
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 2/SMV/SVM(1).R')
plot(svm.lineal5,iris,Petal.Length~Petal.Width)
plot(svm.lineal5,iris)
plot(svm.lineal5, iris, Sepal.Width ~ Sepal.Length,
slice = list(sepal.width = 1, sepal.length = 2))
svm.model <- svm(Species ~ Sepal.Length + Sepal.Width, data = iris, kernel = "linear")
plot(svm.lineal5, iris, Sepal.Width ~ Sepal.Length,
slice = list(sepal.width = 1, sepal.length = 2))
svm.lineal5 <- svm(Species ~ Sepal.Length + Sepal.Width, data = iris, kernel = "linear")
plot(svm.lineal5, iris, Sepal.Width ~ Sepal.Length,
slice = list(sepal.width = 1, sepal.length = 2))
svm.lineal5 <- svm(Species ~ Petal.Length + Petal.Width, data = iris, kernel = "linear")
plot(svm.lineal5, iris, Petal.Width ~ Petal.Length,
slice = list(sepal.width = 1, sepal.length = 2))
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 2/SMV/SVM(1).R')
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 2/SMV/SVM(1).R')
#c)--------------------------------------------------
x1<-c(2,2,-2,-2,2,2,-2,-2,1,1,-1,-1)
x2<-c(2,-2,-2,2,2,-2,-2,2,1,-1,-1,1)
y<-c(1,1,1,1,1,1,1,1,-1,-1,-1,-1)
Data3<-data.frame(x1,x2,y)
x<-cbind(Data3$x1, Data3$x2)
y <- Data3$y
n0 <- sum(y=='1')
n1 <- sum(y=='-1')
colores <- c(rep("green",n0), rep("red",n1))
pchn <- 21
# Diagrama de dispersion
plot(x, pch = pchn, bg = colores)
#Formula, SVM y datos importantes
formula<-as.factor(y)~.
#Como no es lineal, utilizaremos otro kernel
svm.pol <- svm (formula, data=Data3,kernel = "radial", cost = 1000, scale = FALSE)
x.svm <-x [svm.pol$index,]
w <- crossprod(as.matrix(x.svm),as.matrix(svm.pol$coefs))
w0 <- svm.lineal5$rho
#Vectores Soporte
cat("Vectores Soporte (W):\n")
for(i in 1:nrow(x.svm)){
text<-sprintf("(%s)\n",paste(x.svm[i,], collapse=", "))
cat(text)
}
#Valores del kernel
cat("Valores del kernel\n")
show( x.svm%*%t(x.svm) )
#Ancho de banda
text<-sprintf("Ancho de banda: %.5f\n",2/norm(w,type = '2'))
cat(text)
#Vector de pesos normal al hiperplano
text<-sprintf("Vector de pesos normal al hiperplano (W): (%s)\n",paste(w, collapse=", "))
cat(text)
# Termino independiente
text<-sprintf("Termino independiente (B): %.2f\n",w0)
cat(text)
#Ecuaciones de hiperplano
cat("Hiperplanos:\n")
text<-sprintf("%.5fx+%.5fy+%.2f=0\n",w[1],w[2],-w0)
cat(text)
text<-sprintf("%.5fx+%.5fy+%.2f=-1\n",w[1],w[2],-w0)
cat(text)
text<-sprintf("%.5fx+%.5fy+%.2f=1\n",w[1],w[2],-w0)
cat(text)
lim<-x+c(-2,2)
plot(svm.pol,Data3,xlim = c(min(lim[,1]), max(lim[,1])), ylim =c(min(lim[,2]), max(lim[,2])) )
# Diagrama de dispersion
plot(x, pch = pchn, bg = colores)
abline(w0/w[2], -w[1]/w[2])
abline((w0 - 1) / w[2], -w[1] / w[2], lty = 2)
abline((w0 + 1) / w[2], -w[1] / w[2], lty = 2)
plot(x, pch = pchn, bg = colores,xlim = c(min(lim[,1]), max(lim[,1])), ylim =c(min(lim[,2]), max(lim[,2])) )
abline(w0/w[2], -w[1]/w[2])
abline((w0 - 1) / w[2], -w[1] / w[2], lty = 2)
abline((w0 + 1) / w[2], -w[1] / w[2], lty = 2)
w0/w[2]
-w[1]/w[2]
abline(w0, -w[1]/w[2])
abline((w0 - 1) , -w[1] / w[2], lty = 2)
abline((w0 + 1) , -w[1] / w[2], lty = 2)
w0
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 2/SMV/SVM(1).R')
traceback()
w0 <- svm.pol$rho
#Vectores Soporte
cat("Vectores Soporte (W):\n")
for(i in 1:nrow(x.svm)){
text<-sprintf("(%s)\n",paste(x.svm[i,], collapse=", "))
cat(text)
}
#Valores del kernel
cat("Valores del kernel\n")
show( x.svm%*%t(x.svm) )
#Ancho de banda
text<-sprintf("Ancho de banda: %.5f\n",2/norm(w,type = '2'))
cat(text)
#Vector de pesos normal al hiperplano
text<-sprintf("Vector de pesos normal al hiperplano (W): (%s)\n",paste(w, collapse=", "))
cat(text)
# Termino independiente
text<-sprintf("Termino independiente (B): %.2f\n",w0)
cat(text)
#Ecuaciones de hiperplano
cat("Hiperplanos:\n")
text<-sprintf("%.5fx+%.5fy+%.2f=0\n",w[1],w[2],-w0)
cat(text)
text<-sprintf("%.5fx+%.5fy+%.2f=-1\n",w[1],w[2],-w0)
cat(text)
text<-sprintf("%.5fx+%.5fy+%.2f=1\n",w[1],w[2],-w0)
cat(text)
lim<-x+c(-2,2)
plot(x, pch = pchn, bg = colores,xlim = c(min(lim[,1]), max(lim[,1])), ylim =c(min(lim[,2]), max(lim[,2])) )
abline(w0/w[2], -w[1]/w[2])
abline((w0 - 1) / w[2], -w[1] / w[2], lty = 2)
abline((w0 + 1) / w[2], -w[1] / w[2], lty = 2)
w0/w[2]
plot(svm.pol,Data3,xlim = c(min(lim[,1]), max(lim[,1])), ylim =c(min(lim[,2]), max(lim[,2])) )
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 2/SMV/SVM(1).R')
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 2/Titanic/Titanic.R')
# The best one is the SVM. We train it now with all our data:
svm <- svm(Survived ~ Pclass+Sex+Age+SibSp+Parch+Fare+Embarked, trainingData)
# The best one is the SVM. We train it now with all our data:
svm <- svm(Survived ~ Pclass+Sex+Age+SibSp+Parch+Fare+Embarked, DataTrain)
predictions <- round(predict(svm, DataTest))
levels(svm)
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 2/Examen2018/Ejercicio3.R')
abline(w0/w[2], -w[1]/w[2], lwd=2, col='blue')
abline(w0/w[2], -w[1]/w[2], lwd=2, col='blue')
levels(svm.lineal)
summary(svm.lineal)
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 2/SMV/SVM(1).R')
levels(svm.lineal5)
sumarry(svm.lineal5)
summary(svm.lineal5)
# The best one is the SVM. We train it now with all our data:
svm <- svm(Survived ~ Pclass+Sex+Age+SibSp+Parch+Fare+Embarked, DataTrain)
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 2/Titanic/Titanic.R')
summary(svm)
predictions <- round(predict(svm, DataTrain))
# The best one is the SVM. We train it now with all our data:
svm <- svm(Survived ~ ., DataTrain)
predictions <- round(predict(svm, DataTrain))
predictions <- round(predict(svm))
predictions <- round(predict(svm,DataTest))
View(DataTrain)
aux<-DataTrain[,1]
aux.names<-names(DataTrain)
new_x<-DataTrain %>% select(Pclass,everything())
mtcars
new_df <- mtcars %>% select(carb, everything())
DataTrain[,c(2:ncol(DataTrain),1)]
DataTrain <- DataTrain[,c(2:ncol(DataTrain),1)]
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 2/Titanic/Titanic.R')
# The best one is the SVM. We train it now with all our data:
svm <- svm(Survived ~ ., DataTrain)
summary(svm)
predictions <- round(predict(svm,DataTest))
levels(DataTrain)
levels(DataTest)
levels(DataTrain$Embarked)
levels(DataTest$Embarked)
droplevels(DataTrain$Embarked)
DataTrain <- read.csv("Titanic/train.csv")
droplevels(DataTrain$Embarked)
factor(DataTest$Embarked)
source('D:/Users/franc/Google Drive/Ingenieria de Informatica/Curso2018-19/Aprendizaje Computacional/Tema 2/Titanic/Titanic.R')
formula <- Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + as.factor(Embarked)
model.frame(formula, data = DataTrain)
formula <- Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked
model.frame(formula, data = DataTrain)
formula <- Survived ~ .
model.frame(formula, data = DataTrain)
formula <- Survived ~ .-Survived
model.frame(formula, data = DataTrain)
formula <- Survived ~ .-Fare
model.frame(formula, data = DataTrain)
